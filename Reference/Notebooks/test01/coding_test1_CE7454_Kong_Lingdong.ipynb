{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CE7454 : Deep Learning for Data Science\n",
    "##Â Xavier Bresson\n",
    "\n",
    "\n",
    "## Coding Test 1\n",
    "Date: October 7th, 2020<br>\n",
    "\n",
    "*Instructions* <br>\n",
    "Name: Do not forget to add your name to the notebook file \"coding_test1_CE7454_YOUR_NAME.ipynb\".<br>\n",
    "Questions: This notebook has 10 questions.<br>\n",
    "Answers: Write the answers to each question in this notebook.<br>\n",
    "Type: This test is individual and open-book.<br>\n",
    "Grading: 1 point for each question.<br>\n",
    "Delivery: Upload your notebook to https://drive.google.com/drive/folders/1Owos8IyA2Mh2PGvRlTEJZuNmR7sJOpx-  by 6:20pm. <br>\n",
    "Remark: **If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want.**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TT3HcmXNhNxp"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Create a PyTorch tensor $x$ of type FloatTensor, size [5, 2, 7] and filled with random numbers. Print tensor $x$ and its type. Convert the same tensor $x$ to type LongTensor and print its value and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1600109702676,
     "user": {
      "displayName": "Vijay Prakash Dwivedi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgfLL5UlJ0cWGCM7ABRbVFcbAS9vkLq7ias9ewKNA=s64",
      "userId": "03190496352220804755"
     },
     "user_tz": -480
    },
    "id": "vH-tgwa9gf4L",
    "outputId": "22855a52-17af-4e45-d011-3edc6d8f5fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is:\n",
      "tensor([[[11590729.,  9701815., 11257493., 16536800.,  9459967.,  3719045.,\n",
      "          15422661.],\n",
      "         [ 3587402.,  3625892.,  5021539.,  7802040., 10498097.,  7970677.,\n",
      "          10642049.]],\n",
      "\n",
      "        [[ 5167779., 14666898.,  2575403.,    60541., 14871767., 13804029.,\n",
      "            890979.],\n",
      "         [10664355.,  2160860., 12969401.,    85841., 12378694., 12349665.,\n",
      "           8811207.]],\n",
      "\n",
      "        [[ 3276918.,  7054158.,  6056577., 12423720., 10365456.,  8708237.,\n",
      "           7544934.],\n",
      "         [ 6647631.,  3300132.,  6892420.,  3983132.,  3206363.,   108060.,\n",
      "            564177.]],\n",
      "\n",
      "        [[14382251.,  5064879.,  2382149.,  8208201.,  5328591.,  6363700.,\n",
      "          14156344.],\n",
      "         [ 9001799.,  1592680., 10981261.,  5402074., 13383569., 10802019.,\n",
      "           9434792.]],\n",
      "\n",
      "        [[11522983.,   359172.,  4538727., 12086396.,  9029178.,  3124944.,\n",
      "           8590923.],\n",
      "         [13945835.,  4113668., 11464633.,  2919494., 12362805.,  4136984.,\n",
      "          10967238.]]])\n",
      "The type of x is:\n",
      "torch.FloatTensor\n",
      " \n",
      "x is:\n",
      "tensor([[[11590729,  9701815, 11257493, 16536800,  9459967,  3719045, 15422661],\n",
      "         [ 3587402,  3625892,  5021539,  7802040, 10498097,  7970677, 10642049]],\n",
      "\n",
      "        [[ 5167779, 14666898,  2575403,    60541, 14871767, 13804029,   890979],\n",
      "         [10664355,  2160860, 12969401,    85841, 12378694, 12349665,  8811207]],\n",
      "\n",
      "        [[ 3276918,  7054158,  6056577, 12423720, 10365456,  8708237,  7544934],\n",
      "         [ 6647631,  3300132,  6892420,  3983132,  3206363,   108060,   564177]],\n",
      "\n",
      "        [[14382251,  5064879,  2382149,  8208201,  5328591,  6363700, 14156344],\n",
      "         [ 9001799,  1592680, 10981261,  5402074, 13383569, 10802019,  9434792]],\n",
      "\n",
      "        [[11522983,   359172,  4538727, 12086396,  9029178,  3124944,  8590923],\n",
      "         [13945835,  4113668, 11464633,  2919494, 12362805,  4136984, 10967238]]])\n",
      "The type of x is:\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "\n",
    "# YOUR CODE HERE\n",
    "x = torch.FloatTensor(5, 2, 7).random_()\n",
    "print(\"x is:\")\n",
    "print(x)\n",
    "print(\"The type of x is:\")\n",
    "print(x.type())\n",
    "print(\" \")\n",
    "\n",
    "x = x.long()\n",
    "print(\"x is:\")\n",
    "print(x)\n",
    "print(\"The type of x is:\")\n",
    "print(x.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Compute and print the matrix product of matrices A and B given below.\n",
    "\n",
    "Compute and print the element-wise product of matrices C and D given below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix product of matrices A and B is:\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      " \n",
      "The element-wise product of matrices C and D is:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "\n",
    "A=torch.ones(2,3)\n",
    "B=torch.ones(3,2)\n",
    "C=torch.ones(2,2)\n",
    "D=torch.ones(2,2)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "mat_product = torch.mm(A, B)\n",
    "print(\"The matrix product of matrices A and B is:\")\n",
    "print(mat_product)\n",
    "print(\" \")\n",
    "\n",
    "ele_product = torch.mul(C, D)\n",
    "print(\"The element-wise product of matrices C and D is:\")\n",
    "print(ele_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B3tPn7qa2sLk"
   },
   "source": [
    "### Question 3\n",
    "\n",
    "Given a minibatch $x$ of color images represented by a tensor of size (100, 3, 28, 28), with 3 color channels (red, green and blue), a grid domain of 28 pixels by 28 pixels, and 100 images in the minibatch. \n",
    "\n",
    "Transform the tensor $x$ into a vector and print its size.\n",
    "\n",
    "Transform back the vector to a tensor of size (100, 3, 28, 28) and print its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1600108253961,
     "user": {
      "displayName": "Vijay Prakash Dwivedi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgfLL5UlJ0cWGCM7ABRbVFcbAS9vkLq7ias9ewKNA=s64",
      "userId": "03190496352220804755"
     },
     "user_tz": -480
    },
    "id": "djho9-rx3V6Z",
    "outputId": "a7e1e68a-8ce3-427f-8e11-a394c8826bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3, 28, 28)\n",
      "float32\n",
      " \n",
      "torch.Size([100, 3, 28, 28])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "\n",
    "x = torch.rand(100,3,28,28) # given minibatch of color images \n",
    "\n",
    "# YOUR CODE HERE\n",
    "vec = x.numpy()\n",
    "print(vec.shape)\n",
    "print(vec.dtype)\n",
    "print(\" \")\n",
    "\n",
    "tensor = torch.from_numpy(vec)\n",
    "print(tensor.size())\n",
    "print(tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5LNc-Fo7mT1"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Consider a minibatch $s$ of scores of size (100,5) with 5 classes and 100 data points. Convert the scores into probabilities with the softmax operator. Print the scores and the probabilities for the 10th data point (reminder: 1st data point is indexed by 0). \n",
    "\n",
    "Implement a Python function that returns the indices of the classes corresponding to the highest probability for each data in the minibatch. Note that the 5 classes are indexed with integer values $\\{0,1,2,3,4\\}$. Print the index of the highest probability for the 10th data point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1156,
     "status": "ok",
     "timestamp": 1600143105845,
     "user": {
      "displayName": "Vijay Prakash Dwivedi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgfLL5UlJ0cWGCM7ABRbVFcbAS9vkLq7ias9ewKNA=s64",
      "userId": "03190496352220804755"
     },
     "user_tz": -480
    },
    "id": "L67NCs9r7myO",
    "outputId": "ff9a3273-bb24-4cb8-e113-b26ec3bafeed",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores for the 10th data point is:\n",
      "tensor([0.0360, 2.6953, 4.9182, 2.5561, 1.6402])\n",
      "The probabilities for the 10th data point is:\n",
      "tensor([0.0061, 0.0868, 0.8014, 0.0755, 0.0302])\n",
      " \n",
      "The index of the highest probability for the 10th data point is:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongdongdongdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n",
      "/Users/dongdongdongdong/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "\n",
    "s = torch.FloatTensor(100,5).uniform_(-1,5)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import torch.nn.functional as F\n",
    "\n",
    "probs = F.softmax(s)\n",
    "print(\"The scores for the 10th data point is:\")\n",
    "print(s[9])\n",
    "print(\"The probabilities for the 10th data point is:\")\n",
    "print(probs[9])\n",
    "print(\" \")\n",
    "\n",
    "def highest_prob_indice(s):\n",
    "    probs = F.softmax(s)\n",
    "    indice = probs.argmax(dim=1)\n",
    "    return indice\n",
    "\n",
    "print(\"The index of the highest probability for the 10th data point is:\")\n",
    "highest_prob_indice(s)[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZTCUwqn9uYl"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Implement the activation function $$\\sigma(x)=\\frac{2}{2+e^{-x}}$$ as a Python function.\n",
    "\n",
    "Print the activation values of the input tensor x1=$[-4.5,1.2,0.0,6.2]$ with function $\\sigma$. \n",
    "\n",
    "Compute the analytical derivative of $\\sigma$ and implement it as a Python function. \n",
    "\n",
    "$$\\sigma'(x)=\\textrm{to be computed}$$\n",
    "\n",
    "Print the values of the input tensor x2=$[2.3,-1.9,-3.4,6.2]$ with function $\\sigma'$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1943,
     "status": "ok",
     "timestamp": 1600110208955,
     "user": {
      "displayName": "Vijay Prakash Dwivedi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgfLL5UlJ0cWGCM7ABRbVFcbAS9vkLq7ias9ewKNA=s64",
      "userId": "03190496352220804755"
     },
     "user_tz": -480
    },
    "id": "a7MpmJqn9u1N",
    "outputId": "9bd76404-e992-4de6-c145-66b2d28bab86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0217, 0.8691, 0.6667, 0.9990])\n",
      "tensor([0.0455, 0.1772, 0.0587, 0.0010])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([-4.5,1.2,0.0,6.2])\n",
    "x2 = torch.Tensor([2.3,-1.9,-3.4,6.2])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def activation_func(x):\n",
    "    sigma  = 2 / (2 + torch.exp(-x))\n",
    "    return sigma\n",
    "\n",
    "print(activation_func(x1))\n",
    "\n",
    "def derivative_func(x):\n",
    "    partial_sigma = (2 * torch.exp(-x)) / (2 + torch.exp(-x))**2\n",
    "    return partial_sigma\n",
    "\n",
    "print(derivative_func(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_dDEN-vne48"
   },
   "source": [
    "### Question 6\n",
    "\n",
    "Implement a 2-layer MLP with the non-linear activation function $\\sigma$ from Question 5. The output score of the network is defined as\n",
    "\n",
    "$$s = W_2\\sigma(W_1 x + b_1)+b_2$$\n",
    "\n",
    "The size of tensor $x$ is 2 and the size of tensor $s$ is 3.\n",
    "\n",
    "Tensor $W_1$ is a matrix of ones and size (10,2) and tensor $W_2$ is a matrix of ones and size (3,10). \n",
    "\n",
    "Tensors $b_1$ and $b_2$ are vectors of zeros with their sizes constrained by $W_1$ and $W_2$.\n",
    "\n",
    "Print the output tensor $s$ for the input $x=[2.3,-1.4]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1600107092190,
     "user": {
      "displayName": "Vijay Prakash Dwivedi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgfLL5UlJ0cWGCM7ABRbVFcbAS9vkLq7ias9ewKNA=s64",
      "userId": "03190496352220804755"
     },
     "user_tz": -480
    },
    "id": "ZwCjd9Clgjnx",
    "outputId": "3a6d4b04-ab2d-49a9-94a6-6ec527548154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.3106, 8.3106, 8.3106]])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.Tensor([2.3,-1.4])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def activation_func(x):\n",
    "    sigma  = 2 / (2 + torch.exp(-x))\n",
    "    return sigma\n",
    "\n",
    "W1 = torch.ones(10, 2)\n",
    "W2 = torch.ones(3, 10)\n",
    "b1 = torch.zeros(1, 10)\n",
    "b2 = torch.zeros(1, 3)\n",
    "\n",
    "\n",
    "def two_layer_mlp(x):\n",
    "    x = x.view(1, -1)\n",
    "    h = activation_func(x @ W1.T + b1)\n",
    "    h = h @ W2.T + b2\n",
    "    return h\n",
    "\n",
    "s = two_layer_mlp(x)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fAm4Cmrb6z5"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "Given a minibatch $s=[ [5.2,-1.2], [-1.0,4.0], [-1,2] ]$ of scores of three data points and two classes, implement and compute the mean cross entropy for the labels $[0,1,1]$.\n",
    "\n",
    "As a reminder, the definition of the mean cross entropy loss is \n",
    "\n",
    "$$ L = -\\frac{1}{N}\\sum_{i=1}^N \\log \\Big(\\textrm{entry cl($i$) of probability vector } p^{(i)} \\Big)$$\n",
    "\n",
    "where $cl($i$)$ is the class index of the $i^{th}$ training data and $p^{(i)}$ is the probability vector computed by the network (using the score vector).\n",
    "\n",
    "Note that your implementation of the cross entropy cannot use the PyTorch function 'nn.CrossEntropyLoss'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0190)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "s = torch.Tensor( [ [5.2,-1.2], [-1.0,4.0], [-1,2] ] )\n",
    "label = torch.LongTensor([0,1,1])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "\n",
    "def cross_entropy_loss(s, label):\n",
    "    logSoftmax = -torch.log_softmax(s, dim=-1)\n",
    "    entropy = torch.gather(logSoftmax, dim=-1, index=label.view(-1, 1))\n",
    "    return entropy.mean()\n",
    "\n",
    "\n",
    "cross_entropy_loss(s, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Implement a new class for the sigma function $\\sigma$ defined in Question 5. Precisely, define explicitely the forward pass and the backward pass for this new class. Hint: https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "\n",
    "Print the output value $y=\\sigma(x)$ for the tensor $x=[1.6,-1.2,0.0,4.0]$.\n",
    "\n",
    "Consider the loss to be \n",
    "\n",
    "$$ L = \\frac{1}{m}\\sum_{j=1}^m y_j^2,\\quad y_j=\\sigma(x_j)$$\n",
    "\n",
    "where $x_1=1.6, x_2=-1.2, x_3=0.0, x_4=4.0$.\n",
    "\n",
    "Print the gradient value $\\frac{dL}{dx}$ for $x=[1.6,-1.2,0.0,4.0]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "\n",
    "x = torch.Tensor([1.6,-1.2,0.0,4.0])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "class MySigma(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Implement a class of 3-layer MLP with ReLU activation :\n",
    "\n",
    "$$s = W_3\\textrm{ ReLU}(W_2\\textrm{ ReLU}(W_1 x + b_1)+b_2)+b_3)$$\n",
    "\n",
    "Instantiate a network with 784 as input dimension, 50 as hidden dimension and 10 as output dimension.\n",
    "\n",
    "During training, only the last linear layer will be trained (the first two linear layers will not be trained and will keep the same initial values during training). \n",
    "\n",
    "Use 10 epochs to train the network with batch size 100 and learning rate 0.01. Print the loss and error of classification of the training set after 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Epoch: 0, Time: 0.682826042175293, Loss: 2.1406923085451126, Error: 65.5967025756836%\n",
      " \n",
      "Epoch: 1, Time: 1.5185980796813965, Loss: 1.0552668581406275, Error: 24.373321533203125%\n",
      " \n",
      "Epoch: 2, Time: 2.2595200538635254, Loss: 0.5684860005478064, Error: 15.8416748046875%\n",
      " \n",
      "Epoch: 3, Time: 3.2895851135253906, Loss: 0.4580908886839946, Error: 13.09499740600586%\n",
      " \n",
      "Epoch: 4, Time: 4.158290147781372, Loss: 0.40200625131527584, Error: 11.433338165283203%\n",
      " \n",
      "Epoch: 5, Time: 4.83431601524353, Loss: 0.3657472025106351, Error: 10.488338470458984%\n",
      " \n",
      "Epoch: 6, Time: 5.464990139007568, Loss: 0.3403247091174126, Error: 9.775003433227539%\n",
      " \n",
      "Epoch: 7, Time: 6.1218202114105225, Loss: 0.32052154836555324, Error: 9.190007209777832%\n",
      " \n",
      "Epoch: 8, Time: 6.9768030643463135, Loss: 0.304632725790143, Error: 8.685013771057129%\n",
      " \n",
      "Epoch: 9, Time: 7.694217920303345, Loss: 0.2912266111373901, Error: 8.318340301513672%\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import *\n",
    "data_path=check_mnist_dataset_exists()\n",
    "train_data=torch.load(data_path+'mnist/train_data.pt')\n",
    "train_label=torch.load(data_path+'mnist/train_label.pt')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import time\n",
    "\n",
    "data_path = \"/Users/dongdongdongdong/Desktop/data/mnist\"\n",
    "train_data = torch.load(data_path + \"/train_data.pt\")\n",
    "train_label = torch.load(data_path + \"/train_label.pt\")\n",
    "test_data = torch.load(data_path + \"/test_data.pt\")\n",
    "test_label = torch.load(data_path + \"/test_label.pt\")\n",
    "\n",
    "class three_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,  output_size):\n",
    "        super(three_layer_net , self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1  , bias=False  )\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2  , bias=False  )\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size   , bias=False  )        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = F.relu(y)\n",
    "        z       = self.layer2(y_hat)\n",
    "        z_hat   = F.relu(z)\n",
    "        scores  = self.layer3(z_hat)\n",
    "        return scores\n",
    "    \n",
    "    \n",
    "net = three_layer_net(784, 50, 50, 10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "bs = 100\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_error = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    shuffled_indices = torch.randperm(60000)\n",
    "    \n",
    "    for i in range(0, 60000, bs):\n",
    "        \n",
    "        # forward and backward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        indices = shuffled_indices[i : i+bs]\n",
    "        minibatch_data = train_data[indices]\n",
    "        minibatch_labels = train_label[indices]\n",
    "        \n",
    "        inputs = minibatch_data.view(bs, -1)\n",
    "        inputs.requires_grad_()\n",
    "        \n",
    "        scores = net(inputs)\n",
    "        loss = criterion(scores, minibatch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute some stats\n",
    "        running_loss += loss.detach().item()\n",
    "        error = utils.get_error(scores.detach(), minibatch_labels)\n",
    "        running_error += error\n",
    "        num_batches += 1\n",
    "        \n",
    "    # once the epoch is finished we divide the \"running quantities\"\n",
    "    # by the number of batches\n",
    "    total_loss = running_loss / num_batches\n",
    "    total_error = running_error / num_batches\n",
    "    elapsed_time = time.time() - start\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"Epoch: {}, Time: {}, Loss: {}, Error: {}%\".format(\n",
    "            epoch, elapsed_time, total_loss, total_error*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Use the class of 3-layer MLP with ReLU activation implemented in Question 9. \n",
    "\n",
    "We will train the network with the quality loss defined as\n",
    "\n",
    "$$ L = \\Big( \\Pi_{i=1}^N \\textrm{entry cl($i$) of probability vector } p^{(i)} \\Big)^{1/N}$$\n",
    "\n",
    "where $cl($i$)$ is the class index of the $i^{th}$ training data and $p^{(i)}$ is the probability vector computed by the network.\n",
    "\n",
    "Unlike Question 9, all linear layers will be learned during training. \n",
    "\n",
    "Use 10 epochs to train the network with batch size 25 and learning rate 0.01. Print the loss and error of classification of the training set after 10 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Epoch: 0, Time: 1.762186050415039, Loss: 1.0382851350990434, Error: 30.66142463684082%\n",
      " \n",
      "Epoch: 1, Time: 3.907280206680298, Loss: 0.36420674024925875, Error: 10.476632118225098%\n",
      " \n",
      "Epoch: 2, Time: 6.0850749015808105, Loss: 0.30479738157708197, Error: 8.728317260742188%\n",
      " \n",
      "Epoch: 3, Time: 8.055040121078491, Loss: 0.2651481191907078, Error: 7.636672019958496%\n",
      " \n",
      "Epoch: 4, Time: 10.018212080001831, Loss: 0.23385957193677315, Error: 6.626687049865723%\n",
      " \n",
      "Epoch: 5, Time: 12.03337025642395, Loss: 0.20848984520727148, Error: 5.906722545623779%\n",
      " \n",
      "Epoch: 6, Time: 14.009705066680908, Loss: 0.18902616363329192, Error: 5.3834123611450195%\n",
      " \n",
      "Epoch: 7, Time: 16.114185094833374, Loss: 0.17243741815560498, Error: 4.93173885345459%\n",
      " \n",
      "Epoch: 8, Time: 18.112970113754272, Loss: 0.15846718902078769, Error: 4.5367326736450195%\n",
      " \n",
      "Epoch: 9, Time: 20.028197050094604, Loss: 0.14659617592425397, Error: 4.20172643661499%\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import *\n",
    "data_path=check_mnist_dataset_exists()\n",
    "train_data=torch.load(data_path+'mnist/train_data.pt')\n",
    "train_label=torch.load(data_path+'mnist/train_label.pt')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import time\n",
    "\n",
    "data_path = \"/Users/dongdongdongdong/Desktop/data/mnist\"\n",
    "train_data = torch.load(data_path + \"/train_data.pt\")\n",
    "train_label = torch.load(data_path + \"/train_label.pt\")\n",
    "test_data = torch.load(data_path + \"/test_data.pt\")\n",
    "test_label = torch.load(data_path + \"/test_label.pt\")\n",
    "\n",
    "class three_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,  output_size):\n",
    "        super(three_layer_net , self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1  , bias=False  )\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2  , bias=False  )\n",
    "        self.layer3 = nn.Linear(hidden_size2, output_size   , bias=False  )        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = F.relu(y)\n",
    "        z       = self.layer2(y_hat)\n",
    "        z_hat   = F.relu(z)\n",
    "        scores  = self.layer3(z_hat)\n",
    "        return scores\n",
    "    \n",
    "net = three_layer_net(784, 50, 50, 10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "bs = 25\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_error = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    shuffled_indices = torch.randperm(60000)\n",
    "    \n",
    "    for i in range(0, 60000, bs):\n",
    "        \n",
    "        # forward and backward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        indices = shuffled_indices[i : i+bs]\n",
    "        minibatch_data = train_data[indices]\n",
    "        minibatch_labels = train_label[indices]\n",
    "        \n",
    "        inputs = minibatch_data.view(bs, -1)\n",
    "        inputs.requires_grad_()\n",
    "        \n",
    "        scores = net(inputs)\n",
    "        loss = criterion(scores, minibatch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute some stats\n",
    "        running_loss += loss.detach().item()\n",
    "        error = utils.get_error(scores.detach(), minibatch_labels)\n",
    "        running_error += error\n",
    "        num_batches += 1\n",
    "        \n",
    "    # once the epoch is finished we divide the \"running quantities\"\n",
    "    # by the number of batches\n",
    "    total_loss = running_loss / num_batches\n",
    "    total_error = running_error / num_batches\n",
    "    elapsed_time = time.time() - start\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"Epoch: {}, Time: {}, Loss: {}, Error: {}%\".format(\n",
    "            epoch, elapsed_time, total_loss, total_error*100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNd30TjgREYWyAkV0lT4iX6",
   "name": "CE_7454_coding_test_solution_Vijay.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
